{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import igraph as ig\n",
    "import itertools as it\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs sampling inference algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_fast(networks,K,num_samples = 100,skip = 10,n_restarts = 1,burn_in=1,\\\n",
    "                  alpha_guess = None,beta_guess = None,\\\n",
    "                  pi_guess = None,rho_guess = None,\n",
    "                 labels_guess = None,beta_params = [1.,1.,1.,1.,1.,1.],\\\n",
    "                 fix_labels = False,fix_theta = False,\\\n",
    "                  invert = False,fix_rho=False,fix_modes=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        networks: list of igraph graph objects representing the population to be clustered\n",
    "        num_samples: number of Gibbs samples to perform\n",
    "        skip: interval at which to take samples as algorithm runs\n",
    "        n_restarts: number of random restarts\n",
    "        burn_in: thermalization period for Gibbs sampler\n",
    "        alpha_guess: guess for alphas (length K array); used to initialize sampling\n",
    "        beta_guess: guess for betas (length K array); used to initialize sampling\n",
    "        pi_guess: guess for pis (length K array); used to initialize sampling\n",
    "        rho_guess: guess for rho (scalar); used to initialize sampling\n",
    "        labels_guess: guess for cluster assignments (length n array); used to initialize sampling\n",
    "        beta_params: beta prior parameters; [Hu11,Hu01,Hu10,Hu00,astar,bstar] variables from paper\n",
    "        fix_labels: whether or not to fix the cluster assignments as those specified in 'labels_guess'\n",
    "        fix_theta: whether or not to fix the alpha/beta/pi/rho parameters as those specified in the corresponding '_guess' parameters\n",
    "        invert: whethor or not to allow alpha,beta parameters to exceed 0.5 (corresponding to completely random edge flips)\n",
    "        fix_rho: whether or not to fix rho parameter from rho_guess only\n",
    "        fix_modes: 'False' indicates not fixing modes, and otherwise input needs to be dictinary of adjacency matrix arrays for modes\n",
    "        \n",
    "    returns (in order):\n",
    "        posterior mean mode estimators, given by dict of 2D n x n arrays representing adjacency matrices of modes\n",
    "        posterior mode cluster assignments, given by length n array of cluster labels for each sample network\n",
    "        posterior mean theta parameter estimatrs, given by concatenated length 3K vector [alpha_hat,beta_hat,pi_hat]\n",
    "        posterior mean estimator for mode density, given by scalar\n",
    "        mean log probability over samples, given by scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    initialize constants and dictionaries for comparing edge lists quickly\n",
    "    \"\"\"\n",
    "    n = networks[0].vcount()\n",
    "    ms = [g.ecount() for g in networks]\n",
    "    N = len(networks)\n",
    "    nc2 = n*(n-1)/2\n",
    "    edge_dicts = {}\n",
    "    for t,g in enumerate(networks):\n",
    "        edge_dicts[t] = {}\n",
    "        for e in g.es:\n",
    "            edge_dicts[t][tuple(sorted(e.tuple))] = 1\n",
    "    Hu11,Hu01,Hu10,Hu00,astar,bstar = beta_params\n",
    "    \n",
    "    \"\"\"\n",
    "    map single index to (i,j) value of upper triangle of adjacency matrix\n",
    "    used for quick edge flip sampling in mode update step\n",
    "    \"\"\"\n",
    "    def ind2ij(ind,n):\n",
    "        i = n - 2 - np.floor(np.sqrt(-8*ind + 4*n*(n-1)-7)/2.0 - 0.5)\n",
    "        j = ind + i + 1 - n*(n-1)/2 + (n-i)*((n-i)-1)/2\n",
    "        return i,j\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    main loop\n",
    "    \"\"\"\n",
    "    As,gs,thetas,rhos,logPs = [],[],[],[],[]\n",
    "    for restart in range(n_restarts):\n",
    "        \n",
    "        \"\"\"\n",
    "        randomize each parameter according to its prior if no guesses are given\n",
    "        \"\"\"\n",
    "        if alpha_guess is None: alpha_guess = np.random.beta(Hu11,Hu01,size=K)\n",
    "        if beta_guess is None: beta_guess = np.random.beta(Hu10,Hu00,size=K)\n",
    "        if pi_guess is None: pi_guess = np.ones(K)/K\n",
    "        if rho_guess is None: rho_guess = np.random.beta(astar,bstar)\n",
    "        if labels_guess is None: labels_guess = np.random.choice(np.arange(K),size=N,p=pi_guess)\n",
    "        if fix_modes != False:\n",
    "            A = fix_modes.copy()\n",
    "            \n",
    "        alpha = alpha_guess.copy()\n",
    "        beta = beta_guess.copy()\n",
    "        pi = pi_guess.copy()\n",
    "        rho = rho_guess\n",
    "        g = labels_guess.copy()\n",
    "        \n",
    "        \"\"\"\n",
    "        initialize dict of adjacency matrix arrays for modes, array for cluster labels, arrays for theta parameters\n",
    "        the averages of these over all samples in the restart will be obtained as posterior mean estimates of A and theta\n",
    "        argmaxes over samples will be taken for Z estimator\n",
    "        \"\"\"\n",
    "        alpha_hat,beta_hat,pi_hat,rho_hat = np.zeros(K),np.zeros(K),np.zeros(K),0.\n",
    "        A_hat,g_hat = dict.fromkeys(np.arange(K)),[]\n",
    "        for u in range(K):\n",
    "            A_hat[u] = {}\n",
    "        logP_hat = 0.\n",
    "        \n",
    "        \"\"\"\n",
    "        initialize X and N arrays based on initial cluster assignments\n",
    "        \"\"\"\n",
    "        X,Ns = {},np.zeros(K)\n",
    "        for u in range(K):\n",
    "            X[u] = {}\n",
    "            Ns[u] = 0.\n",
    "            for t in range(N):\n",
    "                if g[t] == u:\n",
    "                    for e in edge_dicts[t]:\n",
    "                        if e in X[u]: X[u][e] += 1.\n",
    "                        else: X[u][e] = 1.\n",
    "                    Ns[u] += 1.\n",
    "        Xinv = {}\n",
    "        for u in range(K):\n",
    "            Xinv[u] = {}\n",
    "            for k, v in X[u].items():\n",
    "                Xinv[u][v] = Xinv[u].get(v, []) + [k]\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        take Gibbs samples\n",
    "        \"\"\"\n",
    "        num_sampled = 0.\n",
    "        for samp in range(num_samples*skip + burn_in):\n",
    "            \n",
    "            \"\"\"\n",
    "            sample A if modes are not fixed\n",
    "            uses accelerated sampling scheme described in paper \n",
    "            \"\"\"\n",
    "            if fix_modes == False:\n",
    "\n",
    "                A = {}\n",
    "                for u in range(K):\n",
    "                    A[u] = {}\n",
    "                    total_nonzero = 0\n",
    "                    for l in Xinv[u].keys():\n",
    "                        logdt = np.log(1.-rho) - np.log(rho) + l*np.log(beta[u]) - l*np.log(alpha[u])\\\n",
    "                                + (Ns[u]-l)*np.log(1-beta[u]) - (Ns[u]-l)*np.log(1-alpha[u])\n",
    "                        Q = 1./(1. + np.exp(logdt))\n",
    "                        total_in_class = len(Xinv[u][l])\n",
    "                        total_nonzero += total_in_class\n",
    "                        num2pick = np.random.binomial(total_in_class,p=Q)\n",
    "                        for _ in range(num2pick):\n",
    "                            e = Xinv[u][l].pop()\n",
    "                            A[u][e] = 1\n",
    "                    logdt = np.log(1.-rho) - np.log(rho) \\\n",
    "                                + Ns[u]*np.log(1-beta[u]) - Ns[u]*np.log(1-alpha[u])\n",
    "                    Q = 1./(1. + np.exp(logdt))\n",
    "                    total_zero = nc2 - total_nonzero\n",
    "                    num2pick = np.random.binomial(total_zero,p=Q)\n",
    "                    num_picked = 0\n",
    "                    num_fails = 0\n",
    "                    while (num_picked < num2pick) and (num_fails < 100):\n",
    "                        ind = np.random.randint(int(nc2))\n",
    "                        i,j = ind2ij(ind,n)\n",
    "                        i,j = int(i),int(j)\n",
    "                        if not((i,j) in X[u]) and (i != j): \n",
    "                            A[u][(i,j)] = 1\n",
    "                            num_picked += 1\n",
    "                            num_fails = 0\n",
    "                        else:\n",
    "                            num_fails += 1\n",
    "            \n",
    "            \"\"\"\n",
    "            update Y array based on results from A sampling\n",
    "            \"\"\"\n",
    "            Y = {}\n",
    "            for u in range(K):\n",
    "                Y[u] = {}\n",
    "                for t in range(N):\n",
    "                    Y[u][t] = {'11':0,'10':0,'01':0,'00':0}\n",
    "                    for e in A[u].keys():\n",
    "                        if e in edge_dicts[t]: \n",
    "                            Y[u][t]['11'] += 1.\n",
    "                        else:\n",
    "                            Y[u][t]['01'] += 1.\n",
    "                    Y[u][t]['10'] = ms[t] - Y[u][t]['11']\n",
    "                    Y[u][t]['00'] = nc2 - Y[u][t]['11'] - Y[u][t]['01'] - Y[u][t]['10']       \n",
    "            \n",
    "            \"\"\"\n",
    "            update W array and Mstar based on Y array\n",
    "            \"\"\"\n",
    "            W,Mstar = {},0.\n",
    "            for u in range(K):\n",
    "                W[u] = {'11':0,'10':0,'01':0,'00':0}\n",
    "            for t in range(N):\n",
    "                u = g[t]\n",
    "                for s in ['11','01','10','00']:\n",
    "                    W[u][s] += Y[u][t][s]\n",
    "            for u in range(K):\n",
    "                Mstar += (W[u]['11']+W[u]['01'])/(Ns[u]+1e-100)\n",
    "            \n",
    "            \"\"\"\n",
    "            sample theta parameters if not fixed\n",
    "            \"\"\"\n",
    "            if fix_theta == False:\n",
    "                \n",
    "                for u in range(K):\n",
    "                    alpha[u] = np.random.beta(W[u]['11']+Hu11,W[u]['01']+Hu01)\n",
    "                    beta[u] = np.random.beta(W[u]['10']+Hu10,W[u]['00']+Hu00)\n",
    "                if invert == True:\n",
    "                    for u in range(K):\n",
    "                        alpha[u] = max(alpha[u],1.-alpha[u])\n",
    "                        beta[u] = min(beta[u],1.-beta[u])\n",
    "                    \n",
    "                pi = np.random.dirichlet(1.+Ns)\n",
    "                if fix_rho == True:\n",
    "                    rho = rho_guess\n",
    "                else:\n",
    "                    rho = np.random.beta(astar+Mstar,bstar+K*nc2-Mstar)\n",
    "\n",
    "            \"\"\"\n",
    "            sample cluster labels if not fixed\n",
    "            \"\"\"\n",
    "            if fix_labels == False:\n",
    "\n",
    "                R = {}\n",
    "                for t in range(N):\n",
    "                    R[t] = {}\n",
    "                    logRsum = []\n",
    "                    for u in range(K):\n",
    "                        logR = np.log(pi[u]) + Y[u][t]['11']*np.log(alpha[u])\\\n",
    "                                + Y[u][t]['01']*np.log(1-alpha[u])\\\n",
    "                                + Y[u][t]['10']*np.log(beta[u])\\\n",
    "                                + Y[u][t]['00']*np.log(1-beta[u])\n",
    "                        R[t][u] = logR\n",
    "                        logRsum.append(logR)\n",
    "                    logRsum = logsumexp(logRsum)\n",
    "                    for u in range(K):\n",
    "                        R[t][u] = R[t][u] - logRsum\n",
    "                    g[t] = np.random.choice(np.arange(K),p=[np.exp(R[t][u]) for u in range(K)])\n",
    "\n",
    "                    \n",
    "            \"\"\"\n",
    "            update X and N data structures after cluster assignment sampling\n",
    "            \"\"\"\n",
    "            X,Ns = {},np.zeros(K)\n",
    "            for u in range(K):\n",
    "                X[u] = {}\n",
    "                Ns[u] = 0.\n",
    "                for t in range(N):\n",
    "                    if g[t] == u:\n",
    "                        for e in edge_dicts[t]:\n",
    "                            if e in X[u]: X[u][e] += 1.\n",
    "                            else: X[u][e] = 1.\n",
    "                        Ns[u] += 1.\n",
    "            Xinv = {}\n",
    "            for u in range(K):\n",
    "                Xinv[u] = {}\n",
    "                for k, v in X[u].items():\n",
    "                    Xinv[u][v] = Xinv[u].get(v, []) + [k]\n",
    "            \n",
    "            \"\"\"\n",
    "            take Gibbs sample and update posterior estimators\n",
    "            \"\"\"\n",
    "            if (samp % skip == 0) and (samp > burn_in):\n",
    "                \n",
    "                for u in range(K):\n",
    "                    for e in A[u].keys():\n",
    "                        if e in A_hat[u]: A_hat[u][e] += 1.\n",
    "                        else: A_hat[u][e] = 1.\n",
    "                g_hat.append(g)\n",
    "                alpha_hat += alpha\n",
    "                beta_hat += beta\n",
    "                pi_hat += pi\n",
    "                rho_hat += rho\n",
    "                delta_logP_hat = Mstar*np.log(rho) + (K*nc2-Mstar)*np.log(1.-rho)\n",
    "                for u in range(K):\n",
    "                    delta_logP_hat += Ns[u]*np.log(pi[u]) + W[u]['11']*np.log(alpha[u])\\\n",
    "                                        + W[u]['01']*np.log(1.-alpha[u]) \\\n",
    "                                        + W[u]['10']*np.log(beta[u])\\\n",
    "                                        + W[u]['00']*np.log(1.-beta[u]) \n",
    "                logP_hat += delta_logP_hat\n",
    "                num_sampled += 1.\n",
    "                \n",
    "        \"\"\"\n",
    "        compute final posterior estimators and finish restart\n",
    "        \"\"\"\n",
    "        for u in range(K):\n",
    "            Amat = np.zeros((n,n))\n",
    "            for e in A_hat[u].keys():\n",
    "                A_hat[u][e] = A_hat[u][e]/num_sampled\n",
    "                Amat[e[0],e[1]] = A_hat[u][e]\n",
    "                Amat[e[1],e[0]] = A_hat[u][e]\n",
    "            A_hat[u] = Amat\n",
    "        g_hat = np.array(g_hat)\n",
    "        g_hat = [np.bincount(g_hat[:,i]).argmax() for i in range(N)]\n",
    "        theta_hat = np.concatenate([alpha_hat/num_sampled,beta_hat/num_sampled,\\\n",
    "                                    pi_hat/num_sampled])\n",
    "        rho_hat /= num_sampled\n",
    "        logP_hat /= num_sampled\n",
    "        \n",
    "        As.append(A_hat)\n",
    "        gs.append(g_hat)\n",
    "        thetas.append(theta_hat)\n",
    "        rhos.append(rho_hat)\n",
    "        logPs.append(logP_hat)\n",
    "    \n",
    "    \"\"\"\n",
    "    pick restart with highest average log probability and return posterior estimators for that restart\n",
    "    \"\"\"\n",
    "    best_restart = np.argmax(logPs)\n",
    "    \n",
    "    return As[best_restart],gs[best_restart],thetas[best_restart],rhos[best_restart],logPs[best_restart]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate sample networks from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic(num,modes,alphas,betas,pis):\n",
    "    \n",
    "    \"\"\"\n",
    "    generate networks from model\n",
    "    \n",
    "    inputs:\n",
    "        num: number of sample networks to be drawn\n",
    "        modes: igraph graph objects representing mode networks\n",
    "        alphas: true positive rates for each cluster\n",
    "        betas: false positive rates for each cluster\n",
    "        pis: cluster assignment probabilities for each cluster\n",
    "        \n",
    "    returns (in order):\n",
    "        list of igraph graph objects representing sampled networks\n",
    "        cluster assignments of each sampled network (which mode each was drawn from)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    mode_As = [np.array(g.get_adjacency().data) for g in modes]\n",
    "    K = len(modes)\n",
    "    n = modes[0].vcount()\n",
    "    gs,cluster_labels = [],[]\n",
    "    for t in range(num):\n",
    "        A = np.zeros((n,n))\n",
    "        u = np.random.choice(np.arange(K),p=pis)\n",
    "        Aref,alpha,beta = mode_As[u],alphas[u],betas[u]\n",
    "        cluster_labels.append(u)\n",
    "        for pair in list(it.combinations(np.arange(n),2)):\n",
    "            i,j = pair\n",
    "            r = np.random.rand()\n",
    "            A[i,j] = (Aref[i,j] == 1)*(r < alpha) + (Aref[i,j] == 0)*(r < beta)\n",
    "            A[j,i] = A[i,j]\n",
    "        g = ig.Graph().Adjacency(A.tolist()).as_undirected()\n",
    "        gs.append(g)\n",
    "        \n",
    "    return gs,cluster_labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
